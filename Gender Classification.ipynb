{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the needed Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pip install opencv-python --user\n",
    "! pip install numpy --user\n",
    "! pip install matplotlib --user\n",
    "! pip install pillow --user\n",
    "! pip install matplotlib --user\n",
    "! pip install scikit-image --user\n",
    "! pip install sklearn --user\n",
    "! pip install python-resize-image --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the training Dataset and Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "from PIL import Image\n",
    "from resizeimage import resizeimage\n",
    "import numpy as np\n",
    "\n",
    "# Read RGB images from the training folder\n",
    "# Note if the code is runing on the local PC don not upload all the DATASET upload the apropriate number ONLY!!!\n",
    "# Which is usaally between 2000 to 3000 images !! to prevent PC crash due to RAM Memory Loss !!!!!!!!\n",
    "# To control the Number of Images go to the training Folder and add or delet the files as appropriate\n",
    "females = [cv2.imread(file) for file in glob.glob('Training\\\\female\\\\*.jpg')]\n",
    "males = [cv2.imread(file) for file in glob.glob('Training\\\\male\\\\*.jpg')]\n",
    "print(\"Done Uploading the Images\")\n",
    "\n",
    "#convert the images to greyscale\n",
    "f_gray= [cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) for img in females]\n",
    "m_gray=[cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) for img in males]\n",
    "print(\"Done Converting the Images to Gray\")\n",
    "\n",
    "#histogram equalization\n",
    "f_equlized = [cv2.equalizeHist(img) for img in f_gray]\n",
    "m_equlized = [cv2.equalizeHist(img) for img in m_gray]\n",
    "print(\"Apllied Histogram Equalization on the Gray Images\")\n",
    "\n",
    "#resizing\n",
    "size = [90,90]\n",
    "f_resized = [resizeimage.resize_cover(Image.fromarray(image), size, validate=False) for image in f_equlized]\n",
    "m_resized = [resizeimage.resize_cover(Image.fromarray(image), size, validate=False) for image in m_equlized]\n",
    "print(\"Done Resizing the Histogram Equalized Images to 90*90 Dimensions\")\n",
    "\n",
    "#Convert the resized arrays to numpy array to be used by the Haar Cascade !!\n",
    "f_sized = [np.array(img) for img in f_resized]\n",
    "m_sized = [np.array(img) for img in m_resized]\n",
    "\n",
    "\n",
    "# to view any of the images Whether colerd or Preprocessed Uncomment the next Lines !!!!\n",
    "# AND Change the variable Named (Array_Name) to any of the preferred arrays above !!!!!!\n",
    "#for img in Array_Name:\n",
    "    # Output img with window name as 'image' \n",
    "    #cv2.imshow('image',img)\n",
    "    # Maintain output window utill \n",
    "    # user presses a key \n",
    "    #cv2.waitKey(0)\n",
    "    # Destroying present windows on screen \n",
    "    #cv2.destroyAllWindows()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Apply the Haar Cascade on the Preprocessed Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Detecting the Female Faces\n",
      "Done Detecting the Male Faces\n"
     ]
    }
   ],
   "source": [
    "#Upload the Haar Cascade Model for frontal Face Detection \n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Detect the Faces in the female images\n",
    "f_haar= [face_cascade.detectMultiScale(\n",
    "    img,\n",
    "    scaleFactor=1.1,\n",
    "    minNeighbors=3,\n",
    "    minSize=(30, 30),\n",
    "    flags = cv2.CASCADE_SCALE_IMAGE\n",
    "    ) for img in f_sized]\n",
    "    \n",
    "print(\"Done Detecting the Female Faces\")\n",
    "\n",
    "# Detect the Faces in the male images\n",
    "m_haar=[face_cascade.detectMultiScale(\n",
    "    img,\n",
    "    scaleFactor=1.1,\n",
    "    minNeighbors=3,\n",
    "    minSize=(30, 30),\n",
    "    flags = cv2.CASCADE_SCALE_IMAGE\n",
    ") for img in m_sized]\n",
    "print(\"Done Detecting the Male Faces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the Region of Interest (roi) Detect by the Haar Cascade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2006 faces!\n",
      "Extracted the region of interest for the female faces\n",
      "Extracted the region of interest for the male faces\n"
     ]
    }
   ],
   "source": [
    "# print The Number Of Detected Faces\n",
    "print (\"Found {0} faces!\".format(len(f_haar) + len(m_haar)))\n",
    "\n",
    "# Extract the region of interest in the image (roi)\n",
    "for f in f_haar:\n",
    "        for (x, y, w, h) in f:\n",
    "            f_roi = [img[y:y+h+88, x:x+w+88] for img in f_sized]\n",
    "            \n",
    "print(\"Extracted the region of interest for the female faces\")      \n",
    "\n",
    "for m in m_haar:\n",
    "        for (x, y, w, h) in m:\n",
    "            m_roi = [img[y:y+h+88, x:x+w+88] for img in m_sized]\n",
    "            \n",
    "print(\"Extracted the region of interest for the male faces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To See the region of interest Extracted by the Haar Cascade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in f_roi:            \n",
    "    cv2.imshow(\"Faces found\", img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in m_roi:            \n",
    "    cv2.imshow(\"Faces found\", img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resizing the region of interest for larger Dimensions to Extract More Accurate Featers By the HOG and LBP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Resizing the female faces region of interest to 320*320 Dimensions\n",
      "Done Resizing the male faces region of interest to 320*320 Dimensions\n"
     ]
    }
   ],
   "source": [
    "#resizing\n",
    "size = [320,320]\n",
    "\n",
    "f_resized2 = [resizeimage.resize_cover(Image.fromarray(image), size, validate=False) for image in f_roi]\n",
    "print(\"Done Resizing the female faces region of interest to 320*320 Dimensions\")\n",
    "\n",
    "m_resized2 = [resizeimage.resize_cover(Image.fromarray(image), size, validate=False) for image in m_roi]\n",
    "print(\"Done Resizing the male faces region of interest to 320*320 Dimensions\")\n",
    "\n",
    "#Convert the resized arrays to numpy array to be used by the HOG and LBP !!\n",
    "f_roi2 = [np.array(img) for img in f_resized2]\n",
    "m_roi2 = [np.array(img) for img in m_resized2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### Defining The Function of the Local Binary Patterns (LBPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import feature\n",
    "import numpy as np\n",
    "\n",
    "def describe(image, numPoints, radius, eps=1e-7):\n",
    "    # compute the Local Binary Pattern representation\n",
    "    # of the image, and then use the LBP representation\n",
    "    # to build the histogram of patterns\n",
    "    lbp = feature.local_binary_pattern(image, numPoints, radius, method=\"uniform\")\n",
    "    (hist, _) = np.histogram(lbp.ravel(),\n",
    "    bins=np.arange(0, numPoints + 3),\n",
    "    range=(0, numPoints + 2))\n",
    "    # normalize the histogram\n",
    "    hist = hist.astype(\"float\")\n",
    "    hist /= (hist.sum() + eps)\n",
    "    # return the histogram of Local Binary Patterns\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using HOG and LBP to Extract the features from the Resized Images !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage.feature import hog\n",
    "from skimage import data, exposure\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "from skimage.feature import local_binary_pattern\n",
    "from scipy import sparse\n",
    "\n",
    "data= [] # Array for storing the combined Feature Vectors of the HOG and LBP\n",
    "labels = [] # Array for storing the Labels of the combined Feature Vectors of the HOG and LBP \n",
    "\n",
    "# settings for HOG\n",
    "cs = (3, 3) #Cell Size\n",
    "orn = 8     #Number of Orientations\n",
    "\n",
    "# settings for LBP\n",
    "radius = 3\n",
    "n_points = 8 * radius\n",
    "\n",
    "#Extracting the features of the Female Faces\n",
    "for img in f_roi2:    \n",
    "    #Extract the HOG Feature Vector\n",
    "    fd = hog(img, orientations=orn, pixels_per_cell=cs, cells_per_block=(1, 1), block_norm='L2', feature_vector=True, multichannel=False)\n",
    "    #Extract the LBP Feature Vector\n",
    "    hist = describe(img, n_points, radius)\n",
    "    #Combine Both Features into One Vector\n",
    "    feat = np.hstack([hist, fd])\n",
    "    #add the vector to the Data Array\n",
    "    data.append(feat)\n",
    "    # add the label of the feature in this case 1 for a female Feature\n",
    "    labels.append(1)\n",
    "\n",
    "print(\"DONE Extracting the features of the Female Faces\")\n",
    "\n",
    "#Extracting the features of the Male Faces\n",
    "for img in m_roi2: \n",
    "    #Extract the HOG Feature Vector\n",
    "    fd = hog(img, orientations=orn, pixels_per_cell=cs, cells_per_block=(1, 1), block_norm='L2', feature_vector=True, multichannel=False)\n",
    "    #Extract the LBP Feature Vector\n",
    "    hist = describe(img, n_points, radius)\n",
    "    #Combine Both Features into One Vector\n",
    "    feat = np.hstack([hist, fd])\n",
    "    #add the vector to the Data Array\n",
    "    data.append(feat)\n",
    "    # add the label of the feature in this case 1 for a female Feature\n",
    "    labels.append(0)\n",
    "\n",
    "print(\"DONE Extracting the features of the Female Faces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HOG Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage.feature import hog\n",
    "from skimage import data, exposure\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "\n",
    "i = 0\n",
    "\n",
    "# settings for HOG\n",
    "cs = (3, 3) #Cell Size\n",
    "orn = 8     #Number of Orientations\n",
    "\n",
    "while i < 1:\n",
    "    i = i+1\n",
    "    fd, hog_image = hog(females[i], orientations=orn, pixels_per_cell=cs,\n",
    "                        cells_per_block=(1, 1), visualize=True, multichannel=True)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4), sharex=True, sharey=True)\n",
    "\n",
    "    ax1.axis('off')\n",
    "    ax1.imshow(females[i], cmap=plt.cm.gray)\n",
    "    ax1.set_title('Input image')\n",
    "\n",
    "    # Rescale histogram for better display\n",
    "    hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))\n",
    "\n",
    "    ax2.axis('off')\n",
    "    ax2.imshow(hog_image_rescaled, cmap=plt.cm.gray)\n",
    "    ax2.set_title('Histogram of Oriented Gradients')\n",
    "    plt.show()\n",
    "    \n",
    "i = 0\n",
    "while i < 1:\n",
    "    i = i+1\n",
    "    \n",
    "    fd, hog_image = hog(f_gray[i], orientations=orn, pixels_per_cell=cs,\n",
    "                        cells_per_block=(1, 1), visualize=True, multichannel=False)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4), sharex=True, sharey=True)\n",
    "\n",
    "    ax1.axis('off')\n",
    "    ax1.imshow(f_gray[i], cmap=plt.cm.gray)\n",
    "    ax1.set_title('Gray image')\n",
    "\n",
    "    # Rescale histogram for better display\n",
    "    hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))\n",
    "\n",
    "    ax2.axis('off')\n",
    "    ax2.imshow(hog_image_rescaled, cmap=plt.cm.gray)\n",
    "    ax2.set_title('Histogram of Oriented Gradients')\n",
    "    plt.show()\n",
    "    \n",
    "i = 0\n",
    "while i < 1:\n",
    "    i = i+1\n",
    "    \n",
    "    fd, hog_image = hog(f_resized2[i], orientations=orn, pixels_per_cell=cs,\n",
    "                        cells_per_block=(1, 1), visualize=True, multichannel=False)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4), sharex=True, sharey=True)\n",
    "\n",
    "    ax1.axis('off')\n",
    "    ax1.imshow(f_resized2[i], cmap=plt.cm.gray)\n",
    "    ax1.set_title('Resized image')\n",
    "\n",
    "    # Rescale histogram for better display\n",
    "    hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))\n",
    "\n",
    "    ax2.axis('off')\n",
    "    ax2.imshow(hog_image_rescaled, cmap=plt.cm.gray)\n",
    "    ax2.set_title('Histogram of Oriented Gradients')\n",
    "    plt.show()\n",
    "    \n",
    "i = 0\n",
    "while i < 1:\n",
    "    i = i+1\n",
    "    \n",
    "    fd, hog_image = hog(f_roi2[i], orientations=orn, pixels_per_cell=cs,\n",
    "                        cells_per_block=(1, 1), visualize=True, multichannel=False)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4), sharex=True, sharey=True)\n",
    "\n",
    "    ax1.axis('off')\n",
    "    ax1.imshow(f_roi2[i], cmap=plt.cm.gray)\n",
    "    ax1.set_title('Region of Interest')\n",
    "\n",
    "    # Rescale histogram for better display\n",
    "    hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))\n",
    "\n",
    "    ax2.axis('off')\n",
    "    ax2.imshow(hog_image_rescaled, cmap=plt.cm.gray)\n",
    "    ax2.set_title('Histogram of Oriented Gradients')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Linear Kernel Of the Support Vector Machinr SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Constructing training/testing split...\n",
      " Training Linear SVM classifier...\n",
      " Evaluating classifier on test data ...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.96      0.94       105\n",
      "           1       0.96      0.92      0.94        96\n",
      "\n",
      "    accuracy                           0.94       201\n",
      "   macro avg       0.94      0.94      0.94       201\n",
      "weighted avg       0.94      0.94      0.94       201\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['classification_model.npy']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import SVM\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# Converting the labels from strings to integers\n",
    "labels = [int(label) for label in labels]\n",
    "#Converting the Data array to numpy array to be used by the SVM\n",
    "data = np.array(data, dtype=np.float32)\n",
    "\n",
    "# Partitioning the data into training and testing splits, using 90%\n",
    "# of the data for training and the remaining 10% for testing\n",
    "print(\" Constructing training/testing split...\")\n",
    "(trainData, testData, trainLabels, testLabels) = train_test_split(data, labels, test_size=0.10, random_state=42)\n",
    "\n",
    "\n",
    "#%% Train the linear SVM\n",
    "print(\" Training Linear SVM classifier...\")\n",
    "model = LinearSVC() \n",
    "model.fit(trainData, trainLabels)\n",
    "#%% Evaluate the classifier\n",
    "print(\" Evaluating classifier on test data ...\")\n",
    "predictions = model.predict(testData)\n",
    "print(classification_report(testLabels, predictions))\n",
    "\n",
    "\n",
    "# Save the model:\n",
    "#%% Save the Model\n",
    "joblib.dump(model, 'classification_model.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading and preprocessing new Images for Testing !!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "from PIL import Image\n",
    "from resizeimage import resizeimage\n",
    "import numpy as np\n",
    "\n",
    "# Read RGB images from the validation folder\n",
    "# Note put the images you want to test in the Validation Folder to be uploaded\n",
    "Test = [cv2.imread(file) for file in glob.glob('Validation\\\\Test\\\\*.jpg')]\n",
    "print(\"Done Uploading the Images\")\n",
    "\n",
    "#convert the images to greyscale\n",
    "Test_gray = [cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) for img in Test]\n",
    "print(\"Done Converting the Images to Gray\")\n",
    "\n",
    "#histogram equalization\n",
    "Test_equlized = [cv2.equalizeHist(img) for img in Test_gray]\n",
    "\n",
    "print(\"Apllied Histogram Equalization on the Gray Images\")\n",
    "\n",
    "#resizing\n",
    "size = [90,90]\n",
    "Test_resized = [resizeimage.resize_cover(Image.fromarray(image), size, validate=False) for image in Test_equlized]\n",
    "print(\"Done Resizing the Histogram Equalized Images to 90*90 Dimensions\")\n",
    "\n",
    "#Convert the resized arrays to numpy array to be used by the Haar Cascade !!\n",
    "Test_sized= [np.array(img) for img in Test_resized]\n",
    "\n",
    "# to view any of the images Whether colerd or Preprocessed Uncomment the next Lines !!!!\n",
    "# AND Change the variable Named (Array_Name) to any of the preferred arrays above !!!!!!\n",
    "#for img in Array_Name:\n",
    "    # Output img with window name as 'image' \n",
    "    #cv2.imshow('image',img)\n",
    "    # Maintain output window utill \n",
    "    # user presses a key \n",
    "    #cv2.waitKey(0)\n",
    "    # Destroying present windows on screen \n",
    "    #cv2.destroyAllWindows()\n",
    "\n",
    "#Upload the Haar Cascade Model for frontal Face Detection \n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Detect the Faces in the Test images\n",
    "Test_haar= [face_cascade.detectMultiScale(\n",
    "    img,\n",
    "    scaleFactor=1.1,\n",
    "    minNeighbors=3,\n",
    "    minSize=(30, 30),\n",
    "    flags = cv2.CASCADE_SCALE_IMAGE\n",
    "    ) for img in Test_sized]\n",
    "    \n",
    "print(\"Done Detecting the Test Faces\")\n",
    "\n",
    "   \n",
    "\n",
    "# print The Number Of Detected Faces\n",
    "print (\"Found {0} faces!\".format(len(Test_haar)))\n",
    "\n",
    "# Extract the region of interest in the Test image (roi)\n",
    "for f in Test_haar:\n",
    "        for (x, y, w, h) in f:\n",
    "            Test_roi = [img[y:y+h, x:x+w] for img in Test_sized] \n",
    "            \n",
    "print(\"Extracted the region of interest for the Test faces\")      \n",
    "\n",
    "\n",
    "#resizing\n",
    "size = [320,320]\n",
    "Test_resized2 = [resizeimage.resize_cover(Image.fromarray(image), size, validate=False) for image in Test_roi]\n",
    "\n",
    "Test_roi2 = [np.array(img) for img in Test_resizedt2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To See the region of interest Extracted by the Haar Cascade\n",
    "for img in Test_roi2:            \n",
    "    cv2.imshow(\"Faces found\", img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading the trained SVM Model for predicting the new test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected A Male image\n",
      "Confidence Score [-0.25408373] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.36962039] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.17271571] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.4938392] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.19372816] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.29279273] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.31216451] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.01558686] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.5804139] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.01112624] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.25273307] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.61678031] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.31463339] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.47541545] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.19856366] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.17314578] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.09684652] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.46026529] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.05910205] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.09251275] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.79319954] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.86094132] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.25602981] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.49888721] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.17210865] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.32215251] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.22992917] \n",
      "\n",
      "Detected A Female image\n",
      "Confidence Score [0.87494443] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.13723803] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.74973296] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.791245] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.11105493] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.07378104] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.18690672] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.42487127] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.34697014] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.38000095] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.42939282] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.04061388] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.23903325] \n",
      "\n",
      "Detected A Female image\n",
      "Confidence Score [0.65896911] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.50021124] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.47805144] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.90122513] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.23894465] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.14707864] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.51401217] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.86871111] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.00413159] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.05280402] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.48201784] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.1306036] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.44727734] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.04994817] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.33373907] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.05142131] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.1370176] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.03635543] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.25783382] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.15474743] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.22213475] \n",
      "\n",
      "Detected A Male image\n",
      "Confidence Score [-0.08798893] \n",
      "\n",
      "Females Detected 2 \n",
      "\n",
      "males Detected 60 \n",
      "\n",
      "Not Detected 39 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Upload the Saved svm model:\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage.feature import hog\n",
    "from skimage import data, exposure\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "model = joblib.load('classification_model.npy') # Upload the Model !!!!!!!!!!!!\n",
    "\n",
    "\n",
    "females_d = 0 # number of females detected\n",
    "males_d = 0   # number of males detected\n",
    "\n",
    "# settings for HOG\n",
    "cs = (3, 3) #Cell Size\n",
    "orn = 8     #Number of Orientations\n",
    "\n",
    "# settings for LBP\n",
    "radius = 3\n",
    "n_points = 8 * radius\n",
    "\n",
    "\n",
    "#Extract and predict whether the features belong to a Male or Female Face !!!!!!!!!!!!  \n",
    "                        \n",
    "for img in Test_roi2:\n",
    "    fds = hog(img, orientations=orn, pixels_per_cell=cs, cells_per_block=(1, 1), block_norm='L2', feature_vector=True, multichannel=False)  # extract HOG features from the window captured\n",
    "    hists = describe(img, n_points, radius)\n",
    "    fds = fds.reshape(1, -1) # re shape the vector of HOG\n",
    "    hists = hists.reshape(1, -1)    # re shape the vector of lBP\n",
    "    feat = np.hstack([hists, fds])  #Combine the Features\n",
    "    pred = model.predict(feat) # use the SVM model to make a prediction on the HOG and LBP features extracted from the Image\n",
    "\n",
    "                     \n",
    "    if pred == 1:\n",
    "                if model.decision_function(feat) > 0.6:  # set a threshold value for the SVM prediction i.e. only firm the predictions above probability of 0.6\n",
    "                    females_d +=1\n",
    "                    print(\"Detected A Female image\")\n",
    "                    print(\"Confidence Score {} \\n\".format(model.decision_function(feat)))\n",
    "\n",
    "    if pred == 0:\n",
    "                if model.decision_function(feat) < 0.6:  # set a threshold value for the SVM prediction i.e. only firm the predictions above probability of 0.6\n",
    "                    males_d +=1\n",
    "                    print(\"Detected A Male image\")\n",
    "                    print(\"Confidence Score {} \\n\".format(model.decision_function(feat))) \n",
    "                    \n",
    "print(\"Females Detected {} \\n\".format(females_d))\n",
    "print(\"males Detected {} \\n\".format(males_d))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
